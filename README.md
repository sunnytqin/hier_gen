# Data Drives Unstable Hierarchical Generalization in LMs

**Official Repository**

This is the official code repository for the paper:

**"Data Drives Unstable Hierarchical Generalization in LMs"**
*Tian Qin, Naomi Saphra, David Alvarez-Melis*
EMNLP 2025

ðŸ“„ [Paper](https://aclanthology.org/2025.emnlp-main.593/) | ðŸ”— [ACL Anthology](https://aclanthology.org/2025.emnlp-main.593/)

## Overview

This repository provides code for training transformer models on question formation and tense inflection tasks to study hierarchical generalization in language models. The code enables investigation of how training data complexity and diversity affect whether models learn hierarchical syntactic rules versus linear n-gram-like shortcuts.

## Setup

### 1. Install Dependencies

```bash
# Create a conda/mamba environment (recommended)
conda create -n hiergenv python=3.11
conda activate hiergenv

# Install the local/forked transformers library first
# (adjust path to your local transformers directory)
pip install -e /path/to/transformers

# Install remaining requirements
pip install -r requirements.txt
```

**Note**: This project requires a local/forked version of the `transformers` library (v4.36.0.dev0). Make sure to install it as an editable package before installing other dependencies.

### 2. Configure WandB (Optional)

If you want to use Weights & Biases logging:

```bash
wandb login
```

Update the `WANDB_ENTITY_NAME` in `train_transformers.py` to your WandB entity name (line 25).

To disable WandB logging, don't specify `--run_name` when running the training script.

## Dataset

The repository includes the question formation dataset:
- `data_utils/question_formation_data/question.train` - 100,000 training examples
- `data_utils/question_formation_data/question.val` - 1,000 validation examples
- `data_utils/question_formation_data/question.test` - 10,000 test examples

### Generating Sentence Type Tags

To generate type tag files (`.type` files) that annotate each sentence with its grammatical type structure:

```bash
cd visualization
python generate_sentence_type_tags.py --split test
```

This creates files like `question.test.type` that map each word to its grammatical type (e.g., "the cat" â†’ "d_sg n_sg"). See `visualization/README.md` for more details.

## Training

### Quick Start

Run training with default settings from the original experiment:

```bash
# For CPU/local GPU
python train_transformers.py \
  --callback \
  --batch_size 8 \
  --dataset question_qf_disamb \
  --disamb_num 0 \
  --max_train_steps 300000 \
  --encoder_n_layers 6 \
  --lr 1e-4 \
  --num_warmup_steps 10000 \
  --weight_decay 0.0 \
  --max_grad_norm 1.0 \
  --eval_every 2000 \
  --seed 42 \
  --tied-embedding \
  --run_name qf_disamb_lr1e-4_seed42 \
  --save_every 100000 \
  --save_dir ./checkpoints/ \
  --save_prefix question_qf_disamb_lr1e-4
```

### SLURM Cluster

For SLURM clusters with GPU access:

```bash
sbatch submit_gpu.sh
```

Edit `submit_gpu.sh` to adjust:
- SLURM settings (time, memory, partition, account)
- Training hyperparameters
- Output directory paths

## Model Architecture

The default configuration trains a 6-layer transformer language model with:
- 512-dimensional embeddings
- 8 attention heads
- Tied input/output embeddings
- Standard positional encoding
- Label smoothing: 0.0
- Dropout: 0.1

## Key Arguments

### Dataset Options
- `--dataset`: Dataset name (`question_qf_disamb`, `tense`, etc.)
- `--disamb_num`: Number of disambiguating examples (0 = none)
- `--exclude_identity`: Only include question sentences in training
- `--train_on_decl_only`: Only include declarative sentences

### Model Options
- `--encoder_n_layers`: Number of transformer layers (default: 6)
- `--vec_dim`: Embedding dimension (default: 512)
- `--n_heads`: Number of attention heads (default: 8)
- `--tied-embedding`: Use tied input/output embeddings
- `--gated-model`: Use gated transformer architecture

### Training Options
- `--lr`: Learning rate (default: 1e-4)
- `--batch_size`: Training batch size (default: 8)
- `--max_train_steps`: Maximum training steps (default: 200000)
- `--num_warmup_steps`: Learning rate warmup steps (default: 10000)
- `--weight_decay`: Weight decay coefficient (default: 0.0)
- `--max_grad_norm`: Gradient clipping norm (default: 1.0)

### Evaluation Options
- `--callback`: Enable auxiliary prediction accuracy evaluation
- `--eval_every`: Evaluation frequency in steps (default: 1000)
- `--eval_keys`: Comma-separated eval splits (default: "val,test")

### Checkpointing
- `--save_dir`: Directory to save checkpoints
- `--save_prefix`: Prefix for checkpoint directory names
- `--save_every`: Checkpoint saving frequency (default: 10000)
- `--model_load_path`: Path to load pretrained model

## Output

Training outputs:
- Model checkpoints saved to `{save_dir}/{save_prefix}_{slurm_job_id}/`
- WandB logs (if enabled)
- Training arguments saved as `args.json`

Evaluation metrics:
- Training loss
- Validation/test loss
- Auxiliary prediction accuracy (with `--callback`)

## File Structure

```
hier_gen/
â”œâ”€â”€ train_transformers.py      # Main training script
â”œâ”€â”€ training_utils.py           # Training loop, optimizer, scheduler
â”œâ”€â”€ transformer_helpers.py      # Model creation helpers
â”œâ”€â”€ vocabulary.py               # Vocabulary management
â”œâ”€â”€ collate.py                  # Data batching utilities
â”œâ”€â”€ sequence.py                 # Sequence testing utilities
â”œâ”€â”€ plot.py                     # Plotting utilities
â”œâ”€â”€ util.py                     # General utilities
â”œâ”€â”€ generate_type_tags.py       # Helper functions for type tag generation
â”œâ”€â”€ submit_gpu.sh               # SLURM submission script
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ visualization/              # Data analysis and type tag generation
â”‚   â”œâ”€â”€ generate_sentence_type_tags.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ data_utils/                 # Dataset loading utilities
â”‚   â”œâ”€â”€ lm_dataset_helpers.py
â”‚   â”œâ”€â”€ tense_inflection_helpers.py
â”‚   â””â”€â”€ question_formation_data/
â”œâ”€â”€ models/                     # Model architectures
â”œâ”€â”€ interfaces/                 # Model interfaces
â”œâ”€â”€ layers/                     # Neural network layers
â””â”€â”€ cfgs/                       # Configuration files
```

## Citation

If you use this code in your research, please cite our paper:

```bibtex
@inproceedings{qin-etal-2025-data,
    title = "Data Drives Unstable Hierarchical Generalization in {LM}s",
    author = "Qin, Tian and Saphra, Naomi and Alvarez-Melis, David",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    year = "2025",
    url = "https://aclanthology.org/2025.emnlp-main.593/"
}
```
